{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel==0.37.1 in /usr/local/lib/python3.8/dist-packages (0.37.1)\n",
      "Requirement already satisfied: setuptools==59.6.0 in /usr/local/lib/python3.8/dist-packages (59.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gym[accept-rom-license,atari]==0.21.0 in /usr/local/lib/python3.8/dist-packages (0.21.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.19.0 in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.23.4)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\" in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (0.4.2)\n",
      "Requirement already satisfied: ale-py~=0.7.1; extra == \"atari\" in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (0.7.5)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability==0.19.0) (0.1.8)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability==0.19.0) (0.4.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability==0.19.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow-probability==0.19.0) (1.14.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability==0.19.0) (5.1.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0) (4.65.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0) (5.10.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0) (2.22.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0) (8.1.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license; extra == \"accept-rom-license\" in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0) (0.6.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1; extra == \"atari\"->gym[accept-rom-license,atari]==0.21.0) (5.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources; python_version < \"3.9\"->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.21.0) (3.10.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wheel==0.37.1 setuptools==59.6.0\n",
    "!pip install gym[atari,accept-rom-license]==0.21.0 tensorflow-probability==0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6QUuN_r26wXD"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "from keras.layers import \\\n",
    "    Input, Dense, Reshape, Lambda, Flatten, \\\n",
    "    Conv2D, Conv2DTranspose, Dropout, BatchNormalization\n",
    "\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "KLDivergenceRegularizer = tfp.layers.KLDivergenceRegularizer\n",
    "MultivariateNormalTriL = tfp.layers.MultivariateNormalTriL\n",
    "IndependentBernoulli = tfp.layers.IndependentBernoulli\n",
    "IndependentNormal = tfp.layers.IndependentNormal\n",
    "Independent = tfp.distributions.Independent\n",
    "Bernoulli = tfp.distributions.Bernoulli\n",
    "Normal = tfp.distributions.Normal\n",
    "MultivariateNormalDiag = tfp.distributions.MultivariateNormalDiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DreamerSettings:\n",
    "    action_dims: List[int]\n",
    "    obs_dims: List[int]\n",
    "    repr_dims: List[int]\n",
    "    hidden_dims: List[int]\n",
    "    enc_dims: List[int]\n",
    "    dropout_rate: float = 0.2\n",
    "\n",
    "    @property\n",
    "    def repr_dims_flat(self) -> int:\n",
    "        return self.repr_dims[0] * self.repr_dims[1]\n",
    "\n",
    "    @property\n",
    "    def repr_out_dims_flat(self) -> int:\n",
    "        return self.repr_dims[0] * self.repr_dims[1] + self.hidden_dims[0]\n",
    "\n",
    "    @property\n",
    "    def obs_dims_flat(self) -> int:\n",
    "        return self.obs_dims[0] * self.obs_dims[1] * self.obs_dims[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_obs(env: gym.Env, num_obs: int) -> tf.data.Dataset:\n",
    "    all_obs = []\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    all_obs.append(obs)\n",
    "    while len(all_obs) < num_obs:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        all_obs.append(obs)\n",
    "        if done and len(all_obs) < num_obs:\n",
    "            obs = env.reset()\n",
    "            all_obs.append(obs)\n",
    "    return tf.data.Dataset.from_tensor_slices(tensors=(np.array(all_obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pacman-v5\")\n",
    "settings = DreamerSettings([1], [64, 64, 3], [32, 32], [512], [128])\n",
    "\n",
    "timesteps = 10_000\n",
    "dataset = sample_obs(env, timesteps)\n",
    "dataset = dataset.map(lambda obs: tf.image.resize(obs, settings.obs_dims[:2]))\n",
    "dataset = dataset.map(lambda obs: (obs / 255.0, obs / 255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bwwgL31e7D71"
   },
   "outputs": [],
   "source": [
    "def create_repr_encoder(settings: DreamerSettings) -> Model:\n",
    "    enc_units = MultivariateNormalTriL.params_size(settings.enc_dims[0])\n",
    "    model_in = Input(settings.obs_dims, name=\"enc_out\")\n",
    "    norm_img = Lambda(lambda x: x * 2.0 - 1.0)\n",
    "    batch_norm = BatchNormalization()\n",
    "    cnn_1 = Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"elu\")\n",
    "    cnn_2 = Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"elu\")\n",
    "    cnn_3 = Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"elu\")\n",
    "    cnn_4 = Conv2D(8, (3, 3), padding=\"same\", activation=\"elu\")\n",
    "    drop_1 = Dropout(rate=settings.dropout_rate)\n",
    "    drop_2 = Dropout(rate=settings.dropout_rate)\n",
    "    drop_3 = Dropout(rate=settings.dropout_rate)\n",
    "    drop_4 = Dropout(rate=settings.dropout_rate)\n",
    "    flatten = Flatten()\n",
    "    dense_out = Dense(enc_units, activation=\"linear\", name=\"enc_dense\")\n",
    "    posterior_out = MultivariateNormalTriL(settings.enc_dims[0], name=\"posterior_enc\")\n",
    "\n",
    "    img_in = batch_norm(norm_img(model_in))\n",
    "    prep_model_convs = drop_4(cnn_4(drop_3(cnn_3(drop_2(cnn_2(drop_1(cnn_1(img_in))))))))\n",
    "    model_out = posterior_out(dense_out(flatten(prep_model_convs)))\n",
    "    return Model(inputs=model_in, outputs=model_out, name=\"encoder_model\")\n",
    "\n",
    "\n",
    "def create_repr_decoder(settings: DreamerSettings) -> Model:\n",
    "    image_channels = settings.obs_dims[-1]\n",
    "    upscale_source_dims = (settings.obs_dims[0] // 8 * settings.obs_dims[1] // 8) * 8\n",
    "\n",
    "    post_units = IndependentNormal.params_size(settings.obs_dims_flat)\n",
    "    cov_flat = post_units // (settings.obs_dims_flat // image_channels)\n",
    "\n",
    "    model_in = Input(settings.enc_dims[0], name=\"repr_out\")\n",
    "    dense_in = Dense(upscale_source_dims, activation=\"linear\", name=\"dec_in\")\n",
    "    reshape_in = Reshape((settings.obs_dims[0] // 8, settings.obs_dims[1] // 8, -1))\n",
    "    cnn_1 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"elu\")\n",
    "    cnn_2 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"elu\")\n",
    "    cnn_3 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\"same\", activation=\"elu\")\n",
    "    cnn_4 = Conv2D(64, (3, 3), padding=\"same\", activation=\"elu\")\n",
    "    cnn_5 = Conv2D(cov_flat, (1, 1), padding=\"same\", activation=\"linear\")\n",
    "    drop_1 = Dropout(rate=settings.dropout_rate)\n",
    "    drop_2 = Dropout(rate=settings.dropout_rate)\n",
    "    drop_3 = Dropout(rate=settings.dropout_rate)\n",
    "    flatten = Flatten()\n",
    "    likelihood_out = IndependentNormal(settings.obs_dims)\n",
    "\n",
    "    prep_in = reshape_in(dense_in(model_in))\n",
    "    conv_out = cnn_5(cnn_4(drop_3(cnn_3(drop_2(cnn_2(drop_1(cnn_1(prep_in))))))))\n",
    "    model_out = likelihood_out(flatten(conv_out))\n",
    "    return Model(inputs=model_in, outputs=model_out, name=\"decoder_model\")\n",
    "\n",
    "\n",
    "def compose_models(settings: DreamerSettings) -> Tuple[Model, Model]:\n",
    "    model_in = Input(settings.obs_dims)\n",
    "    encoder = create_repr_encoder(settings)\n",
    "    decoder = create_repr_decoder(settings)\n",
    "    prior = Independent(\n",
    "        Normal(loc=tf.zeros(settings.enc_dims[0]), scale=1),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "\n",
    "    posterior = encoder(model_in)\n",
    "    reconst_dist = decoder(posterior)\n",
    "    model = Model(inputs=[model_in], outputs=[reconst_dist])\n",
    "\n",
    "    likelihood = reconst_dist.log_prob()\n",
    "    divergence = tfd.kl_divergence(posterior, prior)\n",
    "    elbo_loss = tf.reduce_mean(likelihood - divergence)\n",
    "    loss_model = Model(inputs=[model_in], outputs=[elbo_loss])\n",
    "\n",
    "    return model, loss_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6AZ3EsxYHVgb"
   },
   "outputs": [],
   "source": [
    "model, loss = compose_models(settings)\n",
    "model.build([None] + settings.obs_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI1SaAUA7I4W",
    "outputId": "70c94234-f70b-4789-9c80-558a8f9e4723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "141/141 [==============================] - 17s 54ms/step - loss: -2299.9988 - mse: 0.1535 - val_loss: -10089.1543 - val_mse: 0.0403\n",
      "Epoch 2/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -10212.0156 - mse: 0.0339 - val_loss: -16805.6719 - val_mse: 0.0182\n",
      "Epoch 3/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -13969.9014 - mse: 0.0212 - val_loss: -18063.1152 - val_mse: 0.0173\n",
      "Epoch 4/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -16789.4023 - mse: 0.0160 - val_loss: -23547.8184 - val_mse: 0.0092\n",
      "Epoch 5/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -18351.1094 - mse: 0.0136 - val_loss: -21876.7402 - val_mse: 0.0100\n",
      "Epoch 6/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -19092.0156 - mse: 0.0124 - val_loss: -24713.8691 - val_mse: 0.0078\n",
      "Epoch 7/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -19710.9219 - mse: 0.0118 - val_loss: -24601.9707 - val_mse: 0.0077\n",
      "Epoch 8/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -20469.7754 - mse: 0.0109 - val_loss: -25364.1660 - val_mse: 0.0072\n",
      "Epoch 9/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -20088.0762 - mse: 0.0119 - val_loss: -26250.6797 - val_mse: 0.0066\n",
      "Epoch 10/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -21157.2090 - mse: 0.0103 - val_loss: -24386.8125 - val_mse: 0.0084\n",
      "Epoch 11/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -21646.6914 - mse: 0.0099 - val_loss: -26615.1973 - val_mse: 0.0075\n",
      "Epoch 12/500\n",
      "141/141 [==============================] - 6s 44ms/step - loss: -20224.7031 - mse: 0.0138 - val_loss: -25416.9180 - val_mse: 0.0078\n",
      "Epoch 13/500\n",
      "141/141 [==============================] - 7s 43ms/step - loss: -21888.7676 - mse: 0.0096 - val_loss: -28960.3672 - val_mse: 0.0053\n",
      "Epoch 14/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -22642.4648 - mse: 0.0097 - val_loss: -28947.6367 - val_mse: 0.0060\n",
      "Epoch 15/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -23123.7324 - mse: 0.0087 - val_loss: -26648.2656 - val_mse: 0.0068\n",
      "Epoch 16/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -23176.5059 - mse: 0.0086 - val_loss: -29288.1055 - val_mse: 0.0056\n",
      "Epoch 17/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: 123132.7031 - mse: 1.0391 - val_loss: 9727.7578 - val_mse: 0.6462\n",
      "Epoch 18/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: 6544.7368 - mse: 0.4164 - val_loss: -3941.6648 - val_mse: 0.0788\n",
      "Epoch 19/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -4074.0481 - mse: 0.0807 - val_loss: -12602.2959 - val_mse: 0.0237\n",
      "Epoch 20/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -10071.9336 - mse: 0.0319 - val_loss: -17820.5410 - val_mse: 0.0118\n",
      "Epoch 21/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -13801.6465 - mse: 0.0186 - val_loss: -21330.5293 - val_mse: 0.0081\n",
      "Epoch 22/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -16428.3184 - mse: 0.0139 - val_loss: -23473.1738 - val_mse: 0.0070\n",
      "Epoch 23/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -18128.7500 - mse: 0.0121 - val_loss: -24089.8262 - val_mse: 0.0067\n",
      "Epoch 24/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -19353.6836 - mse: 0.0110 - val_loss: -25743.2129 - val_mse: 0.0056\n",
      "Epoch 25/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -19949.5723 - mse: 0.0105 - val_loss: -26476.5293 - val_mse: 0.0055\n",
      "Epoch 26/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -21057.1328 - mse: 0.0096 - val_loss: -21679.5957 - val_mse: 0.0056\n",
      "Epoch 27/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -20622.2051 - mse: 0.0101 - val_loss: -27718.3496 - val_mse: 0.0057\n",
      "Epoch 28/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -22339.3164 - mse: 0.0088 - val_loss: -28229.0723 - val_mse: 0.0047\n",
      "Epoch 29/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -22855.1914 - mse: 0.0083 - val_loss: -27875.7871 - val_mse: 0.0055\n",
      "Epoch 30/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -20722.4590 - mse: 0.0114 - val_loss: -29281.1289 - val_mse: 0.0042\n",
      "Epoch 31/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -21857.4082 - mse: 0.0109 - val_loss: -26577.5117 - val_mse: 0.0049\n",
      "Epoch 32/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -20044.1953 - mse: 0.0144 - val_loss: -24539.0449 - val_mse: 0.0058\n",
      "Epoch 33/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -23415.6074 - mse: 0.0080 - val_loss: -26914.3594 - val_mse: 0.0074\n",
      "Epoch 34/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -24262.5586 - mse: 0.0075 - val_loss: -29687.2090 - val_mse: 0.0050\n",
      "Epoch 35/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -24804.7188 - mse: 0.0073 - val_loss: -30773.4512 - val_mse: 0.0037\n",
      "Epoch 36/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -24836.8945 - mse: 0.0071 - val_loss: -30358.9492 - val_mse: 0.0050\n",
      "Epoch 37/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -13965.8350 - mse: 0.0556 - val_loss: -25366.0703 - val_mse: 0.0060\n",
      "Epoch 38/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -22939.2441 - mse: 0.0075 - val_loss: -29420.1602 - val_mse: 0.0043\n",
      "Epoch 39/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -24817.4238 - mse: 0.0067 - val_loss: -26433.4492 - val_mse: 0.0078\n",
      "Epoch 40/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -25491.6875 - mse: 0.0065 - val_loss: -32060.3281 - val_mse: 0.0038\n",
      "Epoch 41/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -26046.0625 - mse: 0.0065 - val_loss: -30875.1113 - val_mse: 0.0041\n",
      "Epoch 42/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -25088.6289 - mse: 0.0081 - val_loss: -30988.7207 - val_mse: 0.0051\n",
      "Epoch 43/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -26777.8594 - mse: 0.0064 - val_loss: -30682.3457 - val_mse: 0.0048\n",
      "Epoch 44/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -12826.8379 - mse: 0.1449 - val_loss: -27373.3281 - val_mse: 0.0054\n",
      "Epoch 45/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -23974.5898 - mse: 0.0072 - val_loss: -31510.0566 - val_mse: 0.0038\n",
      "Epoch 46/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -24189.6465 - mse: 0.0085 - val_loss: -28954.2227 - val_mse: 0.0062\n",
      "Epoch 47/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -24863.6152 - mse: 0.0069 - val_loss: -32171.1055 - val_mse: 0.0036\n",
      "Epoch 48/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -25023.5410 - mse: 0.0066 - val_loss: -28675.4434 - val_mse: 0.0041\n",
      "Epoch 49/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -26034.7207 - mse: 0.0081 - val_loss: -14152.2490 - val_mse: 0.0268\n",
      "Epoch 50/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -23739.3301 - mse: 0.0085 - val_loss: -29907.2500 - val_mse: 0.0047\n",
      "Epoch 51/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -26162.3184 - mse: 0.0059 - val_loss: -28310.8730 - val_mse: 0.0054\n",
      "Epoch 52/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -23711.7930 - mse: 0.0081 - val_loss: -31282.7969 - val_mse: 0.0034\n",
      "Epoch 53/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -25373.3242 - mse: 0.0072 - val_loss: -32931.7539 - val_mse: 0.0039\n",
      "Epoch 54/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -27369.9961 - mse: 0.0057 - val_loss: -33748.6836 - val_mse: 0.0038\n",
      "Epoch 55/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -24250.1738 - mse: 0.0096 - val_loss: -30892.3184 - val_mse: 0.0041\n",
      "Epoch 56/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -27906.9004 - mse: 0.0055 - val_loss: -29059.8125 - val_mse: 0.0056\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 6s 42ms/step - loss: -5447.3047 - mse: 0.3046 - val_loss: -2415.5342 - val_mse: 0.1826\n",
      "Epoch 58/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -15624.8164 - mse: 0.0423 - val_loss: -28016.7617 - val_mse: 0.0060\n",
      "Epoch 59/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -20404.0449 - mse: 0.0191 - val_loss: -25355.4121 - val_mse: 0.0070\n",
      "Epoch 60/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -25307.1426 - mse: 0.0068 - val_loss: -32005.0352 - val_mse: 0.0040\n",
      "Epoch 61/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -26450.7188 - mse: 0.0059 - val_loss: -33023.9453 - val_mse: 0.0038\n",
      "Epoch 62/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -24379.2344 - mse: 0.0090 - val_loss: -30927.5547 - val_mse: 0.0039\n",
      "Epoch 63/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -28161.7402 - mse: 0.0052 - val_loss: -17614.9180 - val_mse: 0.0181\n",
      "Epoch 64/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -21963.6328 - mse: 0.0150 - val_loss: -29513.0762 - val_mse: 0.0046\n",
      "Epoch 65/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -28169.9707 - mse: 0.0053 - val_loss: -31792.4180 - val_mse: 0.0038\n",
      "Epoch 66/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -28435.2539 - mse: 0.0051 - val_loss: -34574.3359 - val_mse: 0.0035\n",
      "Epoch 67/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -26591.1289 - mse: 0.0064 - val_loss: -30194.6797 - val_mse: 0.0048\n",
      "Epoch 68/500\n",
      "141/141 [==============================] - 6s 44ms/step - loss: -27741.2188 - mse: 0.0065 - val_loss: -30124.2031 - val_mse: 0.0046\n",
      "Epoch 69/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -29762.9082 - mse: 0.0051 - val_loss: -33562.3516 - val_mse: 0.0038\n",
      "Epoch 70/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -28148.8633 - mse: 0.0057 - val_loss: -30173.4043 - val_mse: 0.0032\n",
      "Epoch 71/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -30444.1562 - mse: 0.0046 - val_loss: -31059.3027 - val_mse: 0.0046\n",
      "Epoch 72/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -25097.1328 - mse: 0.0151 - val_loss: -32513.5273 - val_mse: 0.0040\n",
      "Epoch 73/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -26368.4238 - mse: 0.0064 - val_loss: -25798.1914 - val_mse: 0.0082\n",
      "Epoch 74/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -29679.6641 - mse: 0.0051 - val_loss: -34699.8477 - val_mse: 0.0036\n",
      "Epoch 75/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -29742.1328 - mse: 0.0049 - val_loss: -33589.0547 - val_mse: 0.0039\n",
      "Epoch 76/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -28632.2363 - mse: 0.0061 - val_loss: -35112.1758 - val_mse: 0.0033\n",
      "Epoch 77/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -30915.7891 - mse: 0.0050 - val_loss: -34738.6719 - val_mse: 0.0035\n",
      "Epoch 78/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -27679.6328 - mse: 0.0059 - val_loss: -33423.1250 - val_mse: 0.0034\n",
      "Epoch 79/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -31482.9844 - mse: 0.0047 - val_loss: -27825.5430 - val_mse: 0.0035\n",
      "Epoch 80/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -30558.6445 - mse: 0.0052 - val_loss: -35095.3633 - val_mse: 0.0035\n",
      "Epoch 81/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -31340.4023 - mse: 0.0051 - val_loss: -32059.5508 - val_mse: 0.0044\n",
      "Epoch 82/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -32676.6309 - mse: 0.0048 - val_loss: -33171.1289 - val_mse: 0.0033\n",
      "Epoch 83/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -26507.2344 - mse: 0.0088 - val_loss: -31003.0547 - val_mse: 0.0041\n",
      "Epoch 84/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -28064.7383 - mse: 0.0058 - val_loss: -33497.0469 - val_mse: 0.0035\n",
      "Epoch 85/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -30194.0508 - mse: 0.0052 - val_loss: -32470.9688 - val_mse: 0.0041\n",
      "Epoch 86/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -30668.7422 - mse: 0.0050 - val_loss: -32207.4941 - val_mse: 0.0036\n",
      "Epoch 87/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -27260.6074 - mse: 0.0081 - val_loss: -32861.0742 - val_mse: 0.0038\n",
      "Epoch 88/500\n",
      "141/141 [==============================] - 6s 43ms/step - loss: -32017.2402 - mse: 0.0049 - val_loss: -21915.4980 - val_mse: 0.0038\n",
      "Epoch 89/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -29542.7637 - mse: 0.0064 - val_loss: -32491.6953 - val_mse: 0.0039\n",
      "Epoch 90/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -32005.1660 - mse: 0.0049 - val_loss: -19736.3066 - val_mse: 0.0202\n",
      "Epoch 91/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: -21176.3984 - mse: 0.0241 - val_loss: 16102.2666 - val_mse: 0.8840\n",
      "Epoch 92/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 93/500\n",
      "141/141 [==============================] - 6s 42ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 94/500\n",
      "  3/141 [..............................] - ETA: 5s - loss: nan - mse: nan "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred_dist: tf\u001b[38;5;241m.\u001b[39mclip_by_value(\u001b[38;5;241m-\u001b[39my_pred_dist\u001b[38;5;241m.\u001b[39mlog_prob(y_true), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m, \u001b[38;5;241m1e9\u001b[39m)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_eval_batches = ceil((timesteps // batch_size) * 0.1)\n",
    "dataset = dataset.batch(64)\n",
    "train_dataset = dataset.skip(num_eval_batches)\n",
    "eval_dataset = dataset.take(num_eval_batches)\n",
    "train_dataset = train_dataset.shuffle(100)\n",
    "\n",
    "loss = lambda y_true, y_pred_dist: -y_pred_dist.log_prob(y_true)\n",
    "model.compile(optimizer=\"adam\", loss=loss, metrics=[\"mse\"])\n",
    "model.fit(x=train_dataset, epochs=500, validation_data=eval_dataset)\n",
    "model.save_weights(\"vae.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_imgs(x, y=None):\n",
    "    if not isinstance(x, (np.ndarray, np.generic)):\n",
    "        x = np.array(x)\n",
    "    plt.ioff()\n",
    "    n = x.shape[0]\n",
    "    fig, axs = plt.subplots(1, n, figsize=(n, 1))\n",
    "    if y is not None:\n",
    "        fig.suptitle(np.argmax(y, axis=1))\n",
    "    for i in range(n):\n",
    "        axs.flat[i].imshow(x[i].squeeze(), interpolation='none', cmap='gray')\n",
    "        axs.flat[i].axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogPOPWDN8RF6"
   },
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "img_dataset = train_dataset.unbatch().shuffle(100).batch(num_rows)\n",
    "img_in, _ = iter(img_dataset).next()\n",
    "img_out = model(img_in).sample().numpy()\n",
    "img_in = img_in.numpy()\n",
    "img_out = np.clip(img_out, 0.0, 1.0)\n",
    "\n",
    "print(np.max(img_out))\n",
    "print(np.min(img_out))\n",
    "\n",
    "display_imgs(img_in)\n",
    "display_imgs(img_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iQQBk1p8Ym4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
